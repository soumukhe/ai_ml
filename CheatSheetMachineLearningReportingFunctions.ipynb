{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbzagvJfuzm/nrtlgH3GyX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Formulaes and terms"],"metadata":{"id":"EzyfBTaPB-dH"}},{"cell_type":"markdown","source":["## Created by Soumitra, needs review for accuracy\n","\n"," $y_i$ --> is the observed (actual) value of dependent variable. <br>\n"," $\\hat{y}_i$ --> is the predicted value of dependent variable. <br>\n"," $\\bar{y}$ --> mean value for dependentent variable<br>\n"," n --> is the number of observations. <br>\n","$ \\frac{1}{n} \\sum_{i=1}^{n}$ --> Sigma function (totals from $_ith$ to $_n$ instance). <br><br>\n","\n","---\n","**MSE**: (Mean of squared errors: lower is better)<br>\n","\n","**Formulae:** $ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $ ->  actual value compared to predicted value<br>\n","**Definition:** Mean of squared errors  <br>\n","**Units:** Squared units of original data <br>\n","**Interpretation:** Lower values indicate better fit <br>\n","**Practical Use Case:** Evaluating accuracy of regression models. MSE penalizes larger errors more heavily due to squaring. **It is useful for models where large errors should be penalized**. <br>\n","**Other Characteristics:** <br>\n","* MSE is the average of the squared differences between predicted and actual values.\n","* It provides a measure of the average magnitude of errors.\n","* MSE is suitable for comparing different models or tuning hyperparameters.\n","* Lower MSE values indicate better model performance.\n","* MSE values typically range from 0 to positive infinity, with 0 representing perfect fit and higher values indicating worse fit.\n","\n","**When to user MSE as opposed to RMSE:** MSE is commonly used in situations where the focus is on quantifying the magnitude of errors without necessarily needing an interpretable unit of measurement. It is often preferred in optimization problems or when comparing the performance of different models.<br>\n","\n","---\n","\n","**RMSE:** (Root Mean Squared Error (RMSE) measures the average magnitude of the errors between predicted and actual values in a regression model. Its range of values is from 0 to positive infinity. error reporting: lower is better) <br>\n","\n","**Formulae:** $ \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $ -> actual value vs. predicted value<br>\n","**Definition:** Square root of MSE <br>\n","**Units:** Same units as original data <br>\n","**Interpretation:** **Lower values indicate better fit** <br>\n","**Practical Use Case:** Evaluating accuracy of regression models <br>\n","**Other Characteristics:**\n","* RMSE is more sensitive to large errors.\n","It penalizes large errors more heavily than small errors.\n","* RMSE is suitable when you want to prioritize the reduction of large errors and when the error distribution is Gaussian.\n","* Lower RMSE values indicate better model performance.\n","* RMSE values typically range from 0 to positive infinity, with 0 representing perfect fit and higher values indicating worse fit.\n","\n","**When to user RMSE as opposed to MSE:**  is preferred when interpretability of the error metric in the original units of the dependent variable is important. It provides a measure of the average magnitude of errors in the same unit as the dependent variable.\n","\n","---\n","\n","**MAE:** (Mean of absolute errors: lower is better)<br>\n","\n","**Formulae:** $ \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $ -> actual value vs. predicted value <br>\n","**Definition:** Mean of absolute errors <br>\n","**Units:** Same units as original data <br>\n","**Interpretation:** Lower values indicate better fit <br>\n","**Practical Use Case:** Evaluating accuracy of regression models. **It is less sensitive to outliers and provides a more robust measure of error.**\n"," <br>\n"," **Other Characteristics:** <br>\n"," The range of values for Mean Absolute Error (MAE) is from 0 to positive infinity.\n","\n","The minimum value of MAE is 0, which indicates perfect predictions, where the predicted values perfectly match the actual values. As the MAE increases, it indicates that the average absolute difference between the predicted and actual values is also increasing.\n","\n","Since MAE measures the absolute difference between predicted and actual values, it is not affected by the scale of the data. Therefore, the interpretation of MAE values is straightforward: the closer the MAE is to 0, the better the model's predictions.\n","\n","However, unlike metrics such as MSE (Mean Squared Error) or RMSE (Root Mean Squared Error), MAE does not penalize large errors more heavily. As a result, MAE may not be as sensitive to outliers or extreme values in the data, making it a robust measure of model performance in certain situations.\n","\n","---\n","**SSR:** (Sum of squared errors mean value compared to predicted value: Variability explained in regression model: higher is better)<br>\n","\n","**Formulae** $ \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2 $ -> predicted value vs mean<br>\n","**Definition:** Sum of squared errors of the regression, representing the explained sum of squares <br>\n","**Units:** Squared units of original data <br>\n","**Interpretation:** Measures the variability explained by the regression model <br>\n","**Practical Use Case:** Assessing the goodness of fit of the regression model <br>\n","**Other Characteristics:** <br> The Sum of Squared Errors of Regression (SSR) measures the variability explained by the regression model. Its range of values is from 0 to positive infinity.\n","\n","The minimum value of SSR is 0, which would occur if the model perfectly explains all the variability in the dependent variable.\n","As the SSR increases, it indicates that the model's ability to explain the variability in the dependent variable also increases.\n","A higher value of SSR means that a larger proportion of the variability in the dependent variable is accounted for by the regression model.<br>\n","\n","In summary, while there is no upper limit to SSR, a higher value indicates a better fit of the regression model to the data, as it suggests that more of the variability in the dependent variable is being explained by the independent variables in the model. <br>\n","\n","---\n","\n","**SSE** (Sum of squared errors real value compared to predicted value: variability unexplained by the regression model): lower is better <br>\n","\n","**Formulae:** $ \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $-> actual value vs predicted value<br>\n","**Definition:** Sum of squared errors, representing unexplained variance by the model <br>\n","**Units:** Squared units of original data <br>\n","**Interpretation:** Measures the variability unexplained by the model <br>\n","**Practical Use Case:** Assessing the model's ability to explain variance <br>\n","**Other Characteristics:** <br>\n","The Sum of Squared Errors (SSE) measures the unexplained variability by the regression model. Its range of values is from 0 to positive infinity.\n","\n","The minimum value of SSE is 0, which would occur if the regression model perfectly explains all the variability in the dependent variable, leaving no unexplained error.\n","As the SSE increases, it indicates that the model's ability to explain the variability in the dependent variable decreases, resulting in more unexplained error.\n","A lower value of SSE means that less of the variability in the dependent variable remains unexplained by the regression model.<br>\n","\n","In summary, while there is no upper limit to SSE, a lower value indicates a better fit of the regression model to the data, as it suggests that less variability in the dependent variable is unexplained by the independent variables in the model.\n","<br>\n","\n","\n","---\n","\n","**SST:** (Sum of Squared Total: total variance of the dependent variable without considering any relationships with independent variables.: higher means more spread) <br>\n","\n","**Formulae:** $ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $ -> actual value vs mean<br>\n","**Definition:** Total sum of squares, representing the total variance in the observed data <br>\n","**Units:** Squared units of original data <br>\n","**Interpretation:** Measures the total variability in the data <br>\n","**Practical Use Case:** Assessing overall variability in the dataset <br>\n","**Other Characteristics:** <br>\n","he Total Sum of Squares (SST) measures the total variability in the dependent variable. Its range of values is from 0 to positive infinity.\n","\n","The minimum value of SST is 0, which would occur if all observations in the dependent variable have the same value, resulting in no variability.\n","As the SST increases, it indicates that there is more variability in the dependent variable across observations.\n","SST represents the total variance of the dependent variable without considering any relationships with independent variables.\n","In summary, while there is no upper limit to SST, a higher value indicates a greater spread or variability in the dependent variable across observations. It serves as a baseline against which the proportion of variability explained by the regression model (SSR) can be compared.\n","\n","---\n","\n","**R-Squared:** (measures the proportion of the variance in the dependent variable that is explained by the independent variables in the regression model.: 1 is best<br>\n","\n","**Formulae:** $ 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y})^2}{\\sum{i=1}^{n} (y_i - \\bar{y})^2} $   can also be represented as $(1 - \\frac{SSE}{SST})$<br>\n","**Definition:** Proportion of variance explained by the model <br>\n","**Units:** None <br>\n","**Interpretation:** Gives the proportion of variability explained by the model. **Closer to 1 indicates better fit** <br>\n","**Practical Use Case:** Assessing goodness of fit in regression models <br>\n","**Other Characteristics:** <br>\n","The coefficient of determination, commonly referred to as R-squared ($R^2$), measures the proportion of the variance in the dependent variable that is explained by the independent variables in the regression model. Its range of values is from 0 to 1.\n","\n","The minimum value of $R^2$  is 0, indicating that none of the variance in the dependent variable is explained by the independent variables in the model. In other words, the model does not improve upon a simple mean-based prediction.\n","The maximum value of $R^2$  is 1, which occurs when the regression model perfectly predicts the dependent variable based on the independent variables. In this case, all variance in the dependent variable is explained by the model.\n","$R^2$  can also take intermediate values between 0 and 1, indicating the proportion of variance in the dependent variable that is explained by the model. For example, an $R^2$  value of 0.8 indicates that 80% of the variance in the dependent variable is explained by the model, while the remaining 20% is unexplained.\n","In summary,\n","$R^2$  provides a measure of how well the regression model fits the data. A higher $R^2$  value indicates a better fit, as it suggests that a larger proportion of the variance in the dependent variable is explained by the independent variables in the model. However, it's important to interpret $R^2$  in the context of the specific dataset and the goals of the analysis, as it can be influenced by factors such as the number of predictors, the scale of the data, and the presence of outliers.\n","\n","---\n","\n","\n","**Adjusted R-Squared**: (modification of the regular R-squared statistic that adjusts for the number of predictors in a regression model. Its range of values is from negative infinity to 1: 1 is good, meaning model perfectly predicts the dependent variable based on independent variable)<br>\n","\n","**Formulae:** $ 1 - \\frac{{(1 - R^2)(n - 1)}}{{n - k - 1}} $ <br>\n","**$R^2$** is the regular R-squared value. <br>\n","**n**  is the number of observations.\n","**k** is the number of predictors (independent variables) in the model.\n","\n","**Definition:** Adjusted R-squared is a modification of the regular R-squared statistic that adjusts for the number of predictors in a regression model. <br>\n","**Interpretation:** Higher values indicate better fit, but it should be interpreted in the context of the number of predictors in the model. <br>\n","**Practical Use Case:** Evaluating the goodness of fit of regression models with different numbers of predictors. Adjusted R-squared helps identify the most parsimonious model that provides the best balance between explanatory power and model complexity. <br>\n","**Other Characteristics:**\n","- Adjusted R-squared penalizes for including unnecessary predictors in the model.\n","- It takes into account both the number of observations and the number of predictors in the model.\n","- Adjusted R-squared tends to be lower than the regular R-squared, especially when additional predictors are added to the model that do not contribute significantly to explaining the variance in the dependent variable.\n","\n","- Adjusted R-squared **takes into account both the number of observations and the number of predictors in the model**. It penalizes the R-squared value for including unnecessary predictors that may not significantly improve the model's explanatory power. As a result, adjusted R-squared tends to be lower than the regular R-squared, especially when additional predictors are added to the model that do not contribute significantly to explaining the variance in the dependent variable.\n","\n","- Adjusted R-squared is particularly useful when comparing models with different numbers of predictors. It helps identify the most parsimonious model that provides the best balance between explanatory power and model complexity. A higher adjusted R-squared indicates a better fit, but it should be interpreted in the context of the number of predictors in the model.\n","\n","\n","---\n","**MAPE:** (Mean Absolute Percetage error: lower is better)<br>\n","\n","**Formulae:** $ \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100% $ <br>\n","**Definition:** Mean Absolute Percentage Error <br>\n","**Units:** Percentage (%) <br>\n","**Interpretation:** Lower values indicate better fit <br>\n","**Practical Use Case:** Evaluating accuracy of forecasted values in time series analysis. It provides a measure of the average percentage difference between actual and forecasted values, making it interpretable and widely used in forecasting tasks. <br>\n","**Additional Comments:**\n","* MAPE represents the average magnitude of errors as a percentage of the actual values. It is often used in forecasting tasks, such as sales forecasting or demand forecasting, where it provides a measure of the accuracy of the forecasts relative to the actual values.\n","\n","* A lower MAPE indicates better forecast accuracy, with 0 indicating perfect accuracy.\n","* However, MAPE has some limitations, particularly when dealing with small actual values close to zero, as it can lead to undefined or infinite values.\n","* Additionally, MAPE can be sensitive to outliers and can give disproportionate weight to large errors. Despite these limitations, MAPE is widely used in many industries for evaluating forecast accuracy due to its simplicity and interpretability.\n","---"],"metadata":{"id":"hDwj60FtB4m7"}}]}